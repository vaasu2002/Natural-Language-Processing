{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Processing in NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlPiVcIdJFpO"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Supose we have textual data available, we need to apply many of pre-processing steps to the data to transform those words into numerical features that work with machine learning algorithms.\n",
        "\n",
        "The pre-processing steps for the problem depend mainly on the domain and the problem itself.We don't need to apply all the steps for every problem.\n",
        "\n",
        "Here, we're going to see text preprocessing in Python. We'll use NLTK(Natural language toolkit) library here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4JGBS1TJFpP"
      },
      "source": [
        "# import necessary libraries \n",
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdD68bHhJFpT"
      },
      "source": [
        "### Text lowercase\n",
        "\n",
        "We do lowercase the text to reduce the size of the vocabulary of our text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoXbKGUcJFpT",
        "outputId": "e752634e-3d0b-4ba1-86b7-89e83b48f3f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "def lowercase_text(text): \n",
        "    return text.lower() \n",
        "  \n",
        "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
        "lowercase_text(input_str) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weather is too cloudy.possiblity of rain is high,today!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkx5nINIJFpa"
      },
      "source": [
        "### Remove numbers\n",
        "\n",
        "We should either remove the numbers or convert those numbers into textual representations.\n",
        "We use regular expressions(re) to remove the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJxsrbYcJFpc",
        "outputId": "4d490643-fa75-478f-9ced-ab0eb04d7fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# For Removing numbers \n",
        "def remove_num(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result \n",
        "  \n",
        "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
        "remove_num(input_s) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You bought  candies from shop, and  candies are in home.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2dGfGX2JFpg"
      },
      "source": [
        "## Convert the numbers into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafdwS3cJFph",
        "outputId": "b3e2aa86-22b2-4dcc-d249-588d51cb1de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import inflect \n",
        "q = inflect.engine() \n",
        "def convert_num(text): \n",
        "    temp_string = text.split() \n",
        "    new_str = [] \n",
        "    for word in temp_string: \n",
        "        if word.isdigit(): \n",
        "            temp = q.number_to_words(word) \n",
        "            new_str.append(temp) \n",
        "        else: \n",
        "            new_str.append(word) \n",
        "    temp_str = ' '.join(new_str) \n",
        "    return temp_str \n",
        "  \n",
        "input1 = 'I am 20 years old'\n",
        "print(convert_num(input1))\n",
        "input2 = 'I was born in 2002'\n",
        "print(convert_num(input2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am twenty years old\n",
            "I was born in two thousand and two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506D4NxOJFpk"
      },
      "source": [
        "### Remove Punctuation\n",
        "\n",
        "We remove punctuations because of that we don't have different form of the same word. If we don't remove punctuations, then been, been, and been! will be treated separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5eCetpJFpl",
        "outputId": "f1bd340c-2ed5-4eb7-ecf0-f6f6527a2cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# let's remove punctuation \n",
        "def rem_punct(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "  \n",
        "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
        "rem_punct(input_str) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey Are you excited After a week we will be in Shimla'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99YJy-_ZJFpo"
      },
      "source": [
        "### Remove default stopwords:\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0flBIrJFpp",
        "outputId": "927339d9-432a-448b-902c-3aa332cc86ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# importing nltk library\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "  \n",
        "# remove stopwords function \n",
        "def rem_stopwords(text): \n",
        "    stop_words = set(stopwords.words(\"english\")) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text \n",
        "  \n",
        "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
        "rem_stopwords(ex_text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZMQOMO1JFps"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "For Example: Mangoes ---> Mango\n",
        "\n",
        "             Boys ---> Boy\n",
        "             \n",
        "             going ---> go\n",
        "             \n",
        "             \n",
        "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwP4-kAgJFpt",
        "outputId": "be532381-0aac-42fa-9f98-a9b9c3689bbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#importing nltk's porter stemmer \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "stem1 = PorterStemmer() \n",
        "  \n",
        "# stem words in the list of tokenised words \n",
        "def s_words(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    stems = [stem1.stem(word) for word in word_tokens] \n",
        "    return stems \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "s_words(text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3auiVuIJFpw"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YavFewldJFpx",
        "outputId": "4d2a5fc8-8bc7-41b5-e293-b31a5b864f06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.stem import wordnet \n",
        "from nltk.tokenize import word_tokenize \n",
        "lemma = wordnet.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "# lemmatize string \n",
        "def lemmatize_word(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    # provide context i.e. part-of-speech(pos)\n",
        "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
        "    return lemmas \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "lemmatize_word(text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'be',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolution',\n",
              " 'in',\n",
              " 'the',\n",
              " 'World',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individual',\n",
              " 'would',\n",
              " 'generate',\n",
              " 'terabytes',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9vYMcSQYu-",
        "outputId": "7fdb5c11-75b1-4370-b0bb-c608ed4f6d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCjm73RLJFp1",
        "outputId": "04f28ab2-b652-49f8-d85a-28d4b956d005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# importing tokenize library\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  \n",
        "# convert text into word_tokens with their tags \n",
        "def pos_tagg(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    return pos_tag(word_tokens) \n",
        "  \n",
        "pos_tagg('Are you afraid of something?') "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtdatdAsJFp4"
      },
      "source": [
        "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zv_rokfJFp5",
        "outputId": "9eabd906-2851-4544-8788-e0a46b1210e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# downloading the tagset  \n",
        "nltk.download('tagsets') \n",
        "  \n",
        "# extract information about the tag \n",
        "nltk.help.upenn_tagset('PRP')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRBPHRjSJFp8"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzLmybEuJFp8",
        "outputId": "b0937dde-fd78-401a-f21b-30b4fb921b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#importing libraries\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk import pos_tag \n",
        "  \n",
        "# here we define chunking function with text and regular \n",
        "# expressions representing grammar as parameter \n",
        "def chunking(text, grammar): \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # label words with pos \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # create chunk parser using grammar \n",
        "    chunkParser = nltk.RegexpParser(grammar) \n",
        "  \n",
        "    # test it on the list of word tokens with tagged pos \n",
        "    tree = chunkParser.parse(word_pos) \n",
        "      \n",
        "    for subtree in tree.subtrees(): \n",
        "        print(subtree) \n",
        "    #tree.draw() \n",
        "      \n",
        "sentence = 'the little red parrot is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence, grammar) "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYT9PqpwJFqA"
      },
      "source": [
        "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
        "\n",
        "Image after running above code.\n",
        "<img src=\".\\Images\\11.png\">\n",
        "\n",
        "Libraries like Spacy and TextBlob are best for chunking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6faIGsRJFqB"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeA_JtIBJFqC",
        "outputId": "3eb8a1df-6e57-4552-c0fd-a7bf62aa0830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Importing tokenization and chunk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag, ne_chunk \n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "  \n",
        "def ner(text): \n",
        "    # tokenize the text \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # pos tagging of words \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # tree of word entities \n",
        "    print(ne_chunk(word_pos)) \n",
        "  \n",
        "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
        "ner(text) "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  test/NN\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  played/VBD\n",
            "  in/IN\n",
            "  between/IN\n",
            "  (ORGANIZATION WI/NNP)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}