{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk.tokenize.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igiula48FqPM",
        "outputId": "310074c0-be0b-4cf0-8cbb-c8f763df4c3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.This particular tokenizer requires the Punkt sentence tokenization models to be installed. NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.'\n",
        "\n",
        "sent = sent_tokenize(sentence)\n",
        "sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOj-4zmtFvQR",
        "outputId": "2347ac6a-7620-43ba-e912-98de95341ef7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenizers divide strings into lists of substrings.',\n",
              " 'For example, tokenizers can be used to find the words and punctuation in a string.This particular tokenizer requires the Punkt sentence tokenization models to be installed.',\n",
              " 'NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr9Q1_hGFx1g",
        "outputId": "c6f424b0-7f4b-4d42-acb7-8c5654a56462"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sent:\n",
        "  print(word_tokenize(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBIGwjefF1KF",
        "outputId": "e40a6380-298d-4a89-bf56-32a51a358d5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.']\n",
            "['For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string.This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.']\n",
            "['NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = [word_tokenize(t) for t in sent_tokenize(sentence)]\n",
        "word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnU6Xu_jF1O9",
        "outputId": "38e1ce95-b7bb-443d-e6e7-4189a842e49a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.'],\n",
              " ['For',\n",
              "  'example',\n",
              "  ',',\n",
              "  'tokenizers',\n",
              "  'can',\n",
              "  'be',\n",
              "  'used',\n",
              "  'to',\n",
              "  'find',\n",
              "  'the',\n",
              "  'words',\n",
              "  'and',\n",
              "  'punctuation',\n",
              "  'in',\n",
              "  'a',\n",
              "  'string.This',\n",
              "  'particular',\n",
              "  'tokenizer',\n",
              "  'requires',\n",
              "  'the',\n",
              "  'Punkt',\n",
              "  'sentence',\n",
              "  'tokenization',\n",
              "  'models',\n",
              "  'to',\n",
              "  'be',\n",
              "  'installed',\n",
              "  '.'],\n",
              " ['NLTK',\n",
              "  'also',\n",
              "  'provides',\n",
              "  'a',\n",
              "  'simpler',\n",
              "  ',',\n",
              "  'regular-expression',\n",
              "  'based',\n",
              "  'tokenizer',\n",
              "  ',',\n",
              "  'which',\n",
              "  'splits',\n",
              "  'text',\n",
              "  'on',\n",
              "  'whitespace',\n",
              "  'and',\n",
              "  'punctuation',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0KNqtYHFqS4",
        "outputId": "a9bf2f72-cd92-4e33-a12b-d24063ef57c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrCt8QDqFqU6",
        "outputId": "dce2fb84-b688-4be0-84fc-7625088050bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfwavwxjFqW5",
        "outputId": "046a8c53-1355-4521-c59f-eb0dae55c62a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}