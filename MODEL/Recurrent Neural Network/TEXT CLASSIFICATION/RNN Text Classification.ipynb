{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfbe48e-ecc6-4311-8adc-0b9dcbff2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb1d9a3-1ae1-4b1b-ba4e-117772ab1af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0759ca8c-2661-4546-929d-029300146164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3271\n",
      "4342\n"
     ]
    }
   ],
   "source": [
    "print((df.target == 1).sum()) # Disaster\n",
    "print((df.target == 0).sum()) # No Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783fd40-ac38-402e-88e3-dda778faf9c5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8121be-a8c9-4784-9ad8-8c65c57272d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48eeae7a-f254-496f-81b4-624f436a1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "t\n",
      "@bbcmtd Wholesale Markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c285a352-26d9-4dbe-ab58-659d22a634ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df[\"text\"] = df.text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751ded4d-d685-43b4-b485-4458cd56b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vaasu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "#!pip install -q nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7164ef8e-439c-4b14-b209-966cd2c37eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f1b00-1dc9-43df-99f7-14bdb0feb22c",
   "metadata": {},
   "source": [
    "### Forming Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04281c70-c59c-4f78-8340-6a2fc52cf052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17971\n"
     ]
    }
   ],
   "source": [
    "# Counting number of unique words\n",
    "from collections import Counter\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(df.text)\n",
    "\n",
    "num_unique_words = len(counter)\n",
    "print(num_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6043c-0dec-4c96-9ce1-a1540a4f101a",
   "metadata": {},
   "source": [
    "### Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1715a07b-4ea0-4ac8-b290-373e5980b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.9)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "train_sentences = train_df.text.to_numpy()\n",
    "train_labels = train_df.target.to_numpy()\n",
    "val_sentences = val_df.text.to_numpy()\n",
    "val_labels = val_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38623eb0-8224-493a-bcf1-5d18da9a6965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851,), (762,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape, val_sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8963ac-c64c-4e9c-83ce-ff761cc75ea4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc006b82-9ffc-4279-9b43-15bf69aac354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training\n",
    "\n",
    "# each word has unique index\n",
    "word_index = tokenizer.word_index # dict- each word as key and value is unique indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6355e04e-e14f-4ab4-bfe4-12982d790f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "#word_index[\"<START>\"] = 1\n",
    "#word_index[\"<UNK>\"] = 2\n",
    "#word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d89bca-aa7f-49b1-8388-4e05ec8b6345",
   "metadata": {},
   "source": [
    "#### Forming Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947e7e00-163d-4982-aadf-e5caf2c547c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4e2d1e-a125-4f85-9f34-4772827b0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three people died heat wave far'\n",
      " 'haha south tampa getting flooded hah wait second live south tampa gonna gonna fvck flooding'\n",
      " 'raining flooding florida tampabay tampa 18 19 days ive lost count'\n",
      " 'flood bago myanmar arrived bago'\n",
      " 'damage school bus 80 multi car crash breaking']\n",
      "[[463, 8, 437, 168, 358, 486], [750, 511, 2481, 131, 2482, 3090, 554, 529, 112, 511, 2481, 204, 204, 6151, 137], [2483, 137, 2076, 6152, 2481, 1315, 1605, 530, 179, 629, 3091], [114, 4064, 707, 1606, 4064], [125, 94, 334, 4065, 4066, 53, 18, 335]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[10:15])\n",
    "print(train_sequences[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fcc2c-78bc-469d-9fc3-cb25e0b94a1e",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb97681-19b7-4465-be57-f8c201be0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e074d743-14cc-46d4-89be-72a3e9ae0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences = train_sequences,value=word_index[\"<PAD>\"],padding=\"post\",maxlen=max_length,truncating='post')\n",
    "val_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences = val_sequences,value=word_index[\"<PAD>\"],padding=\"post\",maxlen=max_length,truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f4a9-b30c-48bd-b6d4-25b6d4a8d5c3",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82e22b0-75cb-481a-8afa-e7d6a41919f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 16)            287536    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                1568      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 289,137\n",
      "Trainable params: 289,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create RNN model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 16, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 16), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.SimpleRNN(32, dropout=0.9))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7ee424-e051-43f2-a24c-39ef63deb871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaasu\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a952124-32bb-4099-89df-dfabd885afde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 - 3s - loss: 0.6859 - accuracy: 0.5604 - val_loss: 0.6986 - val_accuracy: 0.5341\n",
      "Epoch 2/5\n",
      "215/215 - 1s - loss: 0.6659 - accuracy: 0.5980 - val_loss: 0.6384 - val_accuracy: 0.6457\n",
      "Epoch 3/5\n",
      "215/215 - 1s - loss: 0.6207 - accuracy: 0.6622 - val_loss: 0.5976 - val_accuracy: 0.7034\n",
      "Epoch 4/5\n",
      "215/215 - 2s - loss: 0.5522 - accuracy: 0.7244 - val_loss: 0.5313 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "215/215 - 2s - loss: 0.5179 - accuracy: 0.7551 - val_loss: 0.4939 - val_accuracy: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17cdfc9d970>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_padded, train_labels, epochs=5, validation_data=(val_padded, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d00df-4490-465b-bcc4-358cd3a4d03b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e46a4b34-8d3f-459e-a896-5a736ceb507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_padded)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96896a4-5de3-426a-882e-3a7fc6daf5f8",
   "metadata": {},
   "source": [
    "### Decoding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23f60e41-d96b-4f73-8739-0087503cfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1fab433-bc72-4db9-804e-ca03f7089dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b48cbe-95fc-4e46-968b-5463f0f00548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[463, 8, 437, 168, 358, 486]\n",
      "three people died heat wave far\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "print(train_sequences[10])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c86a5665-c2b1-49f9-a6ed-c32c0531e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three people died heat wave far'\n",
      " 'haha south tampa getting flooded hah wait second live south tampa gonna gonna fvck flooding'\n",
      " 'raining flooding florida tampabay tampa 18 19 days ive lost count'\n",
      " 'flood bago myanmar arrived bago'\n",
      " 'damage school bus 80 multi car crash breaking' 'whats man' 'love fruits'\n",
      " 'summer lovely' 'car fast' 'goooooooaaaaaal']\n",
      "[1 1 1 1 1 0 0 0 0 0]\n",
      "[1, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[10:20])\n",
    "\n",
    "print(train_labels[10:20])\n",
    "print(predictions[10:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
